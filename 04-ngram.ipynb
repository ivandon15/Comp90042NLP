{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *N*-gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be building bigram *n*-gram language models from scratch. The first part of building a language model is collecting counts from corpora. We'll do some preprocessing first, by lowercasing everything and add `<s>` (start) and `</s>` (end) symbols at the beginning and end of each sentence. For bigrams, we are using dictionaries of dictionaries with the strings as keys, which is a convenient though not particularly memory efficient way to represent things. We will use the unigram counts later for doing smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 给所有句子增加前后符号，并小写化\n",
    "def convert_sentence(sentence):\n",
    "    return [\"<s>\"] + [w.lower() for w in sentence] + [\"</s>\"]\n",
    "\n",
    "# 数数句子中单词的数量，并记录bigram和unigram\n",
    "def get_counts(sentences):\n",
    "    bigram_counts = defaultdict(Counter)   # 默认value都是Counter, 即{\"hello\":{'there':2,'!':4},\"bye\":{\"bye\":1,\"!\":3}\n",
    "    unigram_counts = Counter()\n",
    "    start_count = 0  # \"<s>\" counts: need these for bigram probs\n",
    "\n",
    "    # collect initial unigram statistics\n",
    "    for sentence in sentences:\n",
    "        sentence = convert_sentence(sentence)\n",
    "        for word in sentence[1:]: # from 1, so we don't generate the <s> token，即unigram要跳过符号\n",
    "            unigram_counts[word] += 1\n",
    "        start_count += 1 # 每次读过一次句子就统计一个开始符号\n",
    "\n",
    "    # collect bigram counts\n",
    "    for sentence in sentences:\n",
    "        sentence = convert_sentence(sentence)\n",
    "        # generate a list of bigrams\n",
    "        bigram_list = zip(sentence[:-1], sentence[1:]) # !!!!!!!! 把一句话尾巴去掉，zip上这句话把头去掉，这样就错位zip，zip在一起的就是（w_i,w_{i+1}）\n",
    "        # iterate over bigrams\n",
    "        for bigram in bigram_list:\n",
    "            first, second = bigram\n",
    "            bigram_counts[first][second] += 1 # 这里就是拟了联合概率？ 求P(w_i|w_{i-1})=P(w_i,w_{i-1})/P(w_{i-1})\n",
    "            \n",
    "    # 语料库中token的总数\n",
    "    token_count = float(sum(unigram_counts.values()))\n",
    "    return unigram_counts, bigram_counts, start_count, token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 4), ('there', 5), ('nice', 6)]\n"
     ]
    }
   ],
   "source": [
    "l = zip([\"hello\",\"there\",\"nice\",\"to\",\"meet\"][:-1],[4,5,6])\n",
    "print([i for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'collections.Counter'>, {'C': 2, 'D': 2})\n",
      "Counter({'C': 2, 'D': 2})\n"
     ]
    }
   ],
   "source": [
    "a = defaultdict(Counter)\n",
    "d = Counter(\"CCDD\")\n",
    "a.update(d)\n",
    "print(a)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have counts, we can use them to generate sentences. Here we use [numpy.random.choice](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html), which allows us to randomly choose from a list based on a corresponding list of probabilities, which we calculate by normalizing the raw counts. We start with &lt;s&gt;, and generate the next word given the bigram counts which begin with &lt;s&gt;, and then use that word to generate the next word, etc. It stops when it generates an &lt;/s&gt;. We return a string with some fixes to make the sentence look a proper sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import choice \n",
    "\n",
    "def generate_sentence(bigram_counts):\n",
    "    \n",
    "    # 先给句子一个开始符\n",
    "    current_word = \"<s>\"\n",
    "    sentence = [current_word]\n",
    "    \n",
    "    #如果当前的不是句子停止符就接着生成\n",
    "    while current_word != \"</s>\":\n",
    "        # get counts for previous word\n",
    "        prev_word = current_word\n",
    "        # 拿到了所有第一个词是prev_word的字典，字典里面都是跟在prev_word后面的词，以及他们的频率\n",
    "        prev_word_counts = bigram_counts[prev_word]\n",
    "        # obtain bigram probability distribution given the previous word\n",
    "        bigram_probs = []\n",
    "        total_counts = float(sum(prev_word_counts.values())) # 拿到了C(w_{i-1})\n",
    "        for word in prev_word_counts:\n",
    "            bigram_probs.append(prev_word_counts[word] / total_counts) # 得到 P(w_{i-1},w_i)\n",
    "        # sample the next word\n",
    "        # TODO: Randomly choose\n",
    "        current_word = choice(list(prev_word_counts.keys()), p=bigram_probs)# 因为前面的list是每个current word，后面是每个current word的概率，都是对应的\n",
    "        sentence.append(current_word)\n",
    "\n",
    "    # get rid of start and end of sentence tokens\n",
    "    sentence = \" \".join(sentence[1:-1])\n",
    "    return sentence\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our *n*-gram driven sentence generator on samples from two corpora: the Penn Treebank, and some out-of-copyright English literature from Project Gutenberg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading gutenberg: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     Hostname mismatch, certificate is not valid for\n",
      "[nltk_data]     'raw.githubusercontent.com'. (_ssl.c:1123)>\n",
      "[nltk_data] Error loading treebank: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     Hostname mismatch, certificate is not valid for\n",
      "[nltk_data]     'raw.githubusercontent.com'. (_ssl.c:1123)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg, treebank\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'], ['Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing', 'group', '.'], ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank.sents() # 把treebank里面的数据以句子的形式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg\n",
      "Sentence 1\n",
      "the whole stones he formed a garden ----\" she saw her blind and i ' and a most dear jane fairfax ' s very likely , mr .\n",
      "Sentence 2\n",
      "the virtue - morrow -- some to pass , and god , and though there but the weight of all that it settings of our pie ; \" a proper ; and our tongue , among the jews of her husband : 4 : 20 and turned to the waters , leaping and he that people of a little , camping with a louing , earth .\n",
      "Sentence 3\n",
      "36 : 11 : 17 unless , who made an hundred : then dost thou say for my father , why , faint air were their tongue to - trees in their sin .\n",
      "Sentence 4\n",
      "19 : 1 : 41 : 40 and not , be as the son of the monarch reign , suddenly by day of the matured and write thee , and emma fancied myself , then i trust in my request , mammy looked about the shower of turnbull , which , he said unto his grandson .\n",
      "Sentence 5\n",
      "13 allons !\n",
      "Treebank\n",
      "Sentence 1\n",
      "mr. samnick , ` money manager of the kind of 1987 , staff functions ''\n",
      "Sentence 2\n",
      "`` cray 's different items .\n",
      "Sentence 3\n",
      "`` 0 *t*-1 was made the mexico -- and paper .\n",
      "Sentence 4\n",
      "only a positive .\n",
      "Sentence 5\n",
      "criminal lawyers *t*-1 owns the midwest crop year as the bush administration 's short-term direction *-1 following since tightened controls systems because the size of copycats , '' mr. porter .\n"
     ]
    }
   ],
   "source": [
    "gutenberg_unigrams, gutenberg_bigrams, gutenberg_start_count, gutenberg_token_count = get_counts(gutenberg.sents())\n",
    "print(\"Gutenberg\")\n",
    "for i in range(1,6):\n",
    "    print(\"Sentence %d\" % i)\n",
    "    print(generate_sentence(gutenberg_bigrams))\n",
    "    \n",
    "treebank_unigrams, treebank_bigrams, treebank_start_count, treebank_token_count = get_counts(treebank.sents())\n",
    "print(\"Treebank\")\n",
    "for i in range(1,6):\n",
    "    print(\"Sentence %d\" % i)\n",
    "    print(generate_sentence(treebank_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, we can see some local coherence but most of these sentences are complete nonsense. Across the two corpora, the sentences are noticeably different, it's very obvious that the model from Project Gutenberg is trained on literature, whereas the Penn Treebank data is financial. For the latter, there are some strange tokens (those starting with \\*) we should probably have filtered out.\n",
    "\n",
    "Using language models to generate sentences is fun but not very useful.（用模型生成语句很有意思但没啥用） A more practical application is the ability to assign a probability to a sentence（给句子赋概率）. In order to do that for anything but toy examples, however, we will need to smooth our models so it doesn't assign a zero probability to the sentence whenever it sees a bigram. Here, we'll test two fairly simple smoothing techniques, add-*k* smoothing and interpolated smoothing. In both cases, we will calculate the log probability, to avoid working with very small numbers. The functions below give the probability for a single word at index i in a sentence.\n",
    "\n",
    "Notice that interpolation is implemented using 3 probabilities: the bigram, the unigram and a \"zerogram\" probability. The \"zerogram\" actually refers to the probability of any word appearing. We need this extra probability in order to account for out-of-vocabulary (OOVs) words, which result in zero probability for both bigrams and unigrams. Estimating the probability of OOVs is a general problem: here we use an heuristic that uses a uniform distribution over all words in the vocabulary (1 / |V|).\n",
    "在interpolation的时候会涉及到当bigram和unigram都没有的时候，我们会要一个zerogram来避免0概率出现。通常0概率就是OOV的概率，OOV概率又是一个普遍问题，在这里我们用一个启发式方法：用字典里所有词的均匀分布(1 / |V|)来做zeorgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_log_prob_addk(prev_word, word, unigram_counts, bigram_counts, k):\n",
    "    sm_bigram_counts = bigram_counts[prev_word][word] + k # 当前两个词语的组合数量+k\n",
    "    sm_unigram_counts = unigram_counts[prev_word] + k*len(unigram_counts) # 前一个单词出现的总次数再加上k倍的token数量\n",
    "    return math.log(sm_bigram_counts / sm_unigram_counts)\n",
    "\n",
    "# 获得插值结果，对每一个bigram组合进行差值\n",
    "def get_log_prob_interp(prev_word, word, unigram_counts, bigram_counts, start_count, token_count, lambdas):\n",
    "    bigram_lambda = lambdas[0]\n",
    "    unigram_lambda = lambdas[1]\n",
    "    zerogram_lambda = 1 - lambdas[0] - lambdas[1]\n",
    "    \n",
    "    # start by getting bigram probability\n",
    "    sm_bigram_counts = bigram_counts[prev_word][word] * bigram_lambda\n",
    "    if sm_bigram_counts == 0.0:\n",
    "        # 如果当前数量是0，那bigram插值后就是0，为啥要再写一遍？？\n",
    "        # 之所以分开讨论应该是避免prev_word也不存在的情况，这样求概率的时候分母就会为0\n",
    "        interp_bigram_counts = 0\n",
    "    else:\n",
    "        # 如果不是0，先判断前一个词是不是开头，是的话就直接用句子数量\n",
    "        if prev_word == \"<s>\":\n",
    "            u_counts = start_count\n",
    "        else:\n",
    "            # 如果不是开头，通过unigram对这个词的统计，得到前一个单词的数量\n",
    "            u_counts = unigram_counts[prev_word]\n",
    "        # 这里计算的是插值后的两个词组数量，除去prev_word，就是插值后的概率\n",
    "        interp_bigram_counts = sm_bigram_counts / float(u_counts)\n",
    "        \n",
    "    # unigram probability\n",
    "    interp_unigram_counts = (unigram_counts[word] / token_count) * unigram_lambda\n",
    "    \n",
    "    # \"zerogram\" probability: this is to account for out-of-vocabulary words\n",
    "    # this is just 1 / |V|\n",
    "    vocab_size = len(unigram_counts)\n",
    "    interp_zerogram_counts = (1 / float(vocab_size)) * zerogram_lambda\n",
    "    \n",
    "    return math.log(interp_bigram_counts + interp_unigram_counts + interp_zerogram_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending this to calculate the probability of an entire sentence is trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48.460406447015146\n",
      "-49.538820071127226\n",
      "-39.776378681452364\n",
      "-39.245480234555636\n"
     ]
    }
   ],
   "source": [
    "def get_sent_log_prob_addk(sentence, unigram_counts, bigram_counts, start_count, token_count, k):\n",
    "    sentence = convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    \n",
    "    # 给一句话，把这句话里面的概率都求一遍，因为是log所以用加法\n",
    "    return sum([get_log_prob_addk(prev_word, \n",
    "                                  word, \n",
    "                                  unigram_counts, \n",
    "                                  bigram_counts, \n",
    "                                  k) for prev_word, word in bigram_list])\n",
    "\n",
    "def get_sent_log_prob_interp(sentence, unigram_counts, bigram_counts, start_count, token_count, lambdas):\n",
    "    sentence = convert_sentence(sentence)\n",
    "    bigram_list = zip(sentence[:-1], sentence[1:])\n",
    "    return sum([get_log_prob_interp(prev_word, \n",
    "                                    word, \n",
    "                                    unigram_counts, \n",
    "                                    bigram_counts,\n",
    "                                    start_count,\n",
    "                                    token_count, \n",
    "                                    lambdas) for prev_word, word in bigram_list])\n",
    "    \n",
    "sentence = \"revenue increased last quarter .\".split()\n",
    "print(get_sent_log_prob_addk(sentence, gutenberg_unigrams, gutenberg_bigrams, gutenberg_start_count,\n",
    "                             gutenberg_token_count, 0.05))\n",
    "print(get_sent_log_prob_interp(sentence, \n",
    "                               gutenberg_unigrams, \n",
    "                               gutenberg_bigrams, \n",
    "                               gutenberg_start_count, \n",
    "                               gutenberg_token_count, \n",
    "                               (0.8, 0.19)))\n",
    "print(get_sent_log_prob_addk(sentence, treebank_unigrams, treebank_bigrams, treebank_start_count,\n",
    "                             treebank_token_count, 0.05))\n",
    "print(get_sent_log_prob_interp(sentence, \n",
    "                               treebank_unigrams, \n",
    "                               treebank_bigrams, \n",
    "                               treebank_start_count, \n",
    "                               treebank_token_count, \n",
    "                               (0.8, 0.19)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.034507744082557e-18"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.e**(-39.245480234555636)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for our sample sentence looks reasonable, in particular using the Treebank model results in a noticeably higher probability, which is what we'd expect given the input sentence. The differences in probability between the different smoothing techniques is more modest (though keep in mind this is a logrithmic scale). Now, let's use perplexity to evaluate different smoothing techniques at the level of the corpus. For this, we'll use the Brown corpus again, dividing it up randomly into a training set and a test set based on an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\IvanDeng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from random import shuffle\n",
    "nltk.download('brown')\n",
    "\n",
    "sents = list(brown.sents())\n",
    "shuffle(sents)\n",
    "cutoff = int(0.8*len(sents))\n",
    "training_set = sents[:cutoff]\n",
    "test_set = [[word.lower() for word in sent] for sent in sents[cutoff:]]\n",
    "\n",
    "brown_unigrams, brown_bigrams, brown_start_count, brown_token_count = get_counts(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our probabilities are in log space, we will calculate perplexity in log space as well, then take the exponential at the end\n",
    "\n",
    "$PP(W) = \\sqrt[m]{\\frac{1}{P(W)}}$\n",
    "\n",
    "$\\log{PP(W)} = -\\frac{1}{m} \\log{P(W)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.zhihu.com/question/58482430 \\\n",
    "困惑度：语言模型的效果好坏的常用评价指标是困惑度(perplexity),**句子概率越大，语言模型越好，迷惑度越小。**\\\n",
    "log perplexity 可以看作真实分布与预测分布之间的交叉熵 Cross Entropy, 交叉熵描述了两个概率分布之间的一种距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentences, unigram_counts, bigram_counts, start_count, \n",
    "                         token_count, smoothing_function, parameter):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in sentences:\n",
    "        test_token_count += len(sentence) + 1 # have to consider the end token\n",
    "        total_log_prob += smoothing_function(sentence,\n",
    "                                             unigram_counts,\n",
    "                                             bigram_counts,\n",
    "                                             start_count,\n",
    "                                             token_count,\n",
    "                                             parameter)\n",
    "    # 句子的log概率除去当前句子长度\n",
    "    return math.exp(-total_log_prob / test_token_count)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our two smoothing techniques do with a range of possible parameter values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add k\n",
      "0.0001\n",
      "742.8586619814255\n",
      "0.001\n",
      "604.3736239487934\n",
      "0.01\n",
      "712.7983394857446\n",
      "0.05\n",
      "1031.5331251867012\n",
      "0.2\n",
      "1682.9713850153091\n",
      "1.0\n",
      "3497.166991369912\n",
      "interpolation\n",
      "(0.98, 0.010000000000000009)\n",
      "756.4687282790394\n",
      "(0.95, 0.040000000000000036)\n",
      "580.4687517527991\n",
      "(0.75, 0.24)\n",
      "422.2158036564698\n",
      "(0.5, 0.49)\n",
      "419.91144136656897\n",
      "(0.25, 0.74)\n",
      "494.7855394287438\n",
      "(0.001, 0.989)\n",
      "979.3125045486988\n"
     ]
    }
   ],
   "source": [
    "print(\"add k\")\n",
    "for k in [0.0001,0.001,0.01, 0.05,0.2,1.0]:\n",
    "    print(k)\n",
    "    print(calculate_perplexity(test_set,\n",
    "                               brown_unigrams,\n",
    "                               brown_bigrams,\n",
    "                               brown_start_count,\n",
    "                               brown_token_count,\n",
    "                               get_sent_log_prob_addk,\n",
    "                               k))\n",
    "print(\"interpolation\")\n",
    "for bigram_lambda in [0.98,0.95,0.75,0.5,0.25,0.001]:\n",
    "    unigram_lambda = 0.99 - bigram_lambda\n",
    "    lambdas = (bigram_lambda, unigram_lambda)\n",
    "    print(lambdas) \n",
    "    print(calculate_perplexity(test_set, \n",
    "                               brown_unigrams, \n",
    "                               brown_bigrams,\n",
    "                               brown_start_count,\n",
    "                               brown_token_count, \n",
    "                               get_sent_log_prob_interp, \n",
    "                               lambdas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results indicate that, with regards to perplexity, interpolation is generally better than add k. Very low (though not too low) k is preferred for add k, wheres our best lambdas is in the middle of the range, though apparently with a small preference for more weight on the bigram probability, which makes sense.\n",
    "\n",
    "From the basis given here, you can try playing around with some of the other corpora in NLTK and see if you get similar results. You could also implement a trigram model, or another kind of smoothing, to see if you can get better perplexity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
